#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è YAML —Ñ–∞–π–ª—ñ–≤ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É

–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:
    python3 split_for_translation.py <input_file> [--tokens 15000] [--output-dir chunks]

–ü—Ä–∏–∫–ª–∞–¥–∏:
    python3 split_for_translation.py main_menu/localization/english/events_l_english.yml
    python3 split_for_translation.py file.yml --tokens 20000 --output-dir my_chunks
"""
import os
import sys
import argparse
import json
from pathlib import Path

def count_tokens_approx(text):
    """–ü—Ä–∏–±–ª–∏–∑–Ω–∏–π –ø—ñ–¥—Ä–∞—Ö—É–Ω–æ–∫ —Ç–æ–∫–µ–Ω—ñ–≤ (1 —Ç–æ–∫–µ–Ω ‚âà 3 —Å–∏–º–≤–æ–ª–∏ –¥–ª—è –∫–∏—Ä–∏–ª–∏—Ü—ñ)"""
    return len(text) // 3

def split_file(input_file, max_tokens=15000, output_dir=None):
    """–†–æ–∑–¥—ñ–ª—è—î —Ñ–∞–π–ª –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏"""

    # –Ø–∫—â–æ output_dir –Ω–µ –≤–∫–∞–∑–∞–Ω–æ, —Å—Ç–≤–æ—Ä—é—î–º–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É
    if output_dir is None:
        base_name = Path(input_file).stem.replace('_l_english', '')
        output_dir = f"{base_name}_chunks"

    # –°—Ç–≤–æ—Ä—é—î–º–æ –ø–∞–ø–∫—É –¥–ª—è —á–∞—Å—Ç–∏–Ω
    os.makedirs(output_dir, exist_ok=True)

    print(f"üìñ –ß–∏—Ç–∞—é —Ñ–∞–π–ª: {input_file}")
    with open(input_file, 'r', encoding='utf-8-sig') as f:
        lines = f.readlines()

    print(f"üìä –ó–Ω–∞–π–¥–µ–Ω–æ {len(lines)} —Ä—è–¥–∫—ñ–≤")

    # –†–æ–∑–¥—ñ–ª—è—î–º–æ –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
    chunks = []
    current_chunk = []
    current_tokens = 0
    chunk_num = 1

    # –î–æ–¥–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ –ø–µ—Ä—à–æ—ó —á–∞—Å—Ç–∏–Ω–∏
    header = lines[0]

    for i, line in enumerate(lines[1:], 1):  # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        line_tokens = count_tokens_approx(line)

        if current_tokens + line_tokens > max_tokens and current_chunk:
            # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ç–æ—á–Ω—É —á–∞—Å—Ç–∏–Ω—É
            chunk_file = os.path.join(output_dir, f'chunk_{chunk_num:03d}.txt')

            # –ü–µ—Ä—à–∞ —á–∞—Å—Ç–∏–Ω–∞ –º–∞—î –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if chunk_num == 1:
                chunk_content = header + ''.join(current_chunk)
            else:
                chunk_content = ''.join(current_chunk)

            with open(chunk_file, 'w', encoding='utf-8-sig') as f:
                f.write(chunk_content)

            chunks.append({
                'num': chunk_num,
                'file': chunk_file,
                'lines': len(current_chunk),
                'tokens': current_tokens
            })

            print(f"‚úÖ –ß–∞—Å—Ç–∏–Ω–∞ {chunk_num}: {len(current_chunk)} —Ä—è–¥–∫—ñ–≤, ~{current_tokens} —Ç–æ–∫–µ–Ω—ñ–≤")

            # –ü–æ—á–∏–Ω–∞—î–º–æ –Ω–æ–≤—É —á–∞—Å—Ç–∏–Ω—É
            chunk_num += 1
            current_chunk = []
            current_tokens = 0

        current_chunk.append(line)
        current_tokens += line_tokens

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ—Å—Ç–∞–Ω–Ω—é —á–∞—Å—Ç–∏–Ω—É
    if current_chunk:
        chunk_file = os.path.join(output_dir, f'chunk_{chunk_num:03d}.txt')

        if chunk_num == 1:
            chunk_content = header + ''.join(current_chunk)
        else:
            chunk_content = ''.join(current_chunk)

        with open(chunk_file, 'w', encoding='utf-8-sig') as f:
            f.write(chunk_content)

        chunks.append({
            'num': chunk_num,
            'file': chunk_file,
            'lines': len(current_chunk),
            'tokens': current_tokens
        })

        print(f"‚úÖ –ß–∞—Å—Ç–∏–Ω–∞ {chunk_num}: {len(current_chunk)} —Ä—è–¥–∫—ñ–≤, ~{current_tokens} —Ç–æ–∫–µ–Ω—ñ–≤")

    print(f"\nüéâ –†–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–∏–Ω!")
    print(f"üìÅ –ß–∞—Å—Ç–∏–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –≤: {output_dir}/")

    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–µ—Ç–∞–¥–∞–Ω—ñ
    metadata = {
        'input_file': input_file,
        'output_dir': output_dir,
        'total_chunks': len(chunks),
        'max_tokens': max_tokens,
        'chunks': chunks
    }

    metadata_file = os.path.join(output_dir, 'metadata.json')
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ: {metadata_file}")

    # –°—Ç–≤–æ—Ä—é—î–º–æ —ñ–Ω—Å—Ç—Ä—É–∫—Ü—ñ—é
    instruction_file = os.path.join(output_dir, '–Ü–ù–°–¢–†–£–ö–¶–Ü–Ø.txt')
    with open(instruction_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–Ü–ù–°–¢–†–£–ö–¶–Ü–Ø –ó –ü–ï–†–ï–ö–õ–ê–î–£\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"–§–∞–π–ª: {input_file}\n")
        f.write(f"–†–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–∏–Ω.\n\n")
        f.write("–ö—Ä–æ–∫–∏ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É:\n")
        f.write("1. –í—ñ–¥–∫—Ä–∏–π –∫–æ–∂–µ–Ω —Ñ–∞–π–ª chunk_XXX.txt\n")
        f.write("2. –ü–µ—Ä–µ–∫–ª–∞–¥–∏ –π–æ–≥–æ (–≤—Ä—É—á–Ω—É –∞–±–æ —á–µ—Ä–µ–∑ Claude)\n")
        f.write("3. –ó–±–µ—Ä–µ–∂–∏ –ø–µ—Ä–µ–∫–ª–∞–¥ —è–∫ chunk_XXX_fixed.txt (–∞–±–æ chunk_XXX_translated.txt)\n")
        f.write("4. –ü—ñ—Å–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É –≤—Å—ñ—Ö —á–∞—Å—Ç–∏–Ω –∑–∞–ø—É—Å—Ç–∏:\n")
        f.write(f"   python3 tools/merge_translation.py {output_dir}\n\n")
        f.write("–°–ø–∏—Å–æ–∫ —á–∞—Å—Ç–∏–Ω:\n")
        for chunk in chunks:
            f.write(f"  - {chunk['file']} ({chunk['lines']} —Ä—è–¥–∫—ñ–≤, ~{chunk['tokens']} —Ç–æ–∫–µ–Ω—ñ–≤)\n")

    print(f"üìã –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {instruction_file}")

    return chunks

def main():
    parser = argparse.ArgumentParser(
        description='–†–æ–∑–¥—ñ–ª–∏—Ç–∏ YAML —Ñ–∞–π–ª –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
–ü—Ä–∏–∫–ª–∞–¥–∏:
  %(prog)s events_l_english.yml
  %(prog)s file.yml --tokens 20000
  %(prog)s file.yml --output-dir my_chunks
        '''
    )

    parser.add_argument('input_file', help='–í—Ö—ñ–¥–Ω–∏–π YAML —Ñ–∞–π–ª')
    parser.add_argument('--tokens', type=int, default=15000,
                       help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–∫–µ–Ω—ñ–≤ –Ω–∞ —á–∞—Å—Ç–∏–Ω—É (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: 15000)')
    parser.add_argument('--output-dir', help='–ü–∞–ø–∫–∞ –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —á–∞—Å—Ç–∏–Ω (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º: <filename>_chunks)')

    args = parser.parse_args()

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É
    if not os.path.exists(args.input_file):
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {args.input_file}")
        sys.exit(1)

    chunks = split_file(args.input_file, max_tokens=args.tokens, output_dir=args.output_dir)

    print("\n" + "=" * 80)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("=" * 80)
    print(f"\n–†–æ–∑–¥—ñ–ª–µ–Ω–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–∏–Ω.")
    output_dir = args.output_dir or f"{Path(args.input_file).stem.replace('_l_english', '')}_chunks"
    print(f"–ß–∞—Å—Ç–∏–Ω–∏ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –≤ –ø–∞–ø—Ü—ñ: {output_dir}/")
    print(f"\n–ù–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫: –ø–µ—Ä–µ–∫–ª–∞–¥–∏ –≤—Å—ñ chunk_XXX.txt —Ç–∞ –∑–±–µ—Ä–µ–∂–∏ —è–∫ chunk_XXX_fixed.txt")
    print(f"–ü–æ—Ç—ñ–º –∑–∞–ø—É—Å—Ç–∏: python3 tools/merge_translation.py {output_dir}")

if __name__ == '__main__':
    main()
